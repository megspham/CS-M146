{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW4-CodingQuestions",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Problem 3: K-Means Algorithm"
      ],
      "metadata": {
        "id": "5jhNkul8NxvA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gNicxsYS2sf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Function to load the CIFAR10 data\n",
        "## Documentation of CIFAR10: https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "def dataloader():\n",
        "  import tensorflow as tf\n",
        "  cifar10 = tf.keras.datasets.cifar10\n",
        "  (_, _), (X, y) = cifar10.load_data()\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "DagYciWRTpKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## simple utility function to visualize the data\n",
        "def visualize(X, ind):\n",
        "  from PIL import Image \n",
        "  plt.imshow(Image.fromarray(X[ind], 'RGB'))"
      ],
      "metadata": {
        "id": "lUBxH5COTpWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = dataloader()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eARNIUlTq0r",
        "outputId": "2d74e45f-4fb1-431e-b654-905e1d083a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 3s 0us/step\n",
            "170508288/170498071 [==============================] - 3s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10K images of size 32 x 32 x 3 \n",
        "# where 32 x 32 is the height and width of the image\n",
        "# 3 is the number of channels 'RGB'\n",
        "X.shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdhStn3ETsp3",
        "outputId": "26d7b2f5-afab-4132-ee94-66832caa52ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10000, 32, 32, 3), (10000, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize(X, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "YlWk76g4Tt4E",
        "outputId": "266bfdcf-ce9b-4fe0-8eb5-21da9fe0e157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  Implement this function to form a 10000 x N matrix \n",
        "  from 10000 x 32 x 32 x 3 shape input.\n",
        "'''\n",
        "def reshape(X):\n",
        "  '''\n",
        "    Write one line of code here\n",
        "  '''\n",
        "  return X"
      ],
      "metadata": {
        "id": "stl1q-S1Tvdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = reshape(X)"
      ],
      "metadata": {
        "id": "usmQSj8yT4PD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clustering_score = []\n",
        "for i in tqdm(range(5, 20, 5)):\n",
        "  score = 0\n",
        "  for rs in tqdm(range(3)):\n",
        "    kmeans = KMeans(n_clusters = i, init = 'random', random_state = rs)\n",
        "    '''\n",
        "      Write one line of code to fit the kMeans algorithm to the data\n",
        "      Write another line of code to report the kMeans clustering score defined as\n",
        "      Sum of squared distances of samples to their closest cluster center, weighted by the sample weights if provided.\n",
        "      Hint: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
        "    '''\n",
        "  clustering_score.append(score/3) ## divide by 3 because 3 random states"
      ],
      "metadata": {
        "id": "dtSNFihHT67F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  Submit the plot you get after running this piece of code in your solutions\n",
        "'''\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(range(5, 20, 5), clustering_score)\n",
        "plt.xlabel('No. of Clusters')\n",
        "plt.ylabel('Clustering Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "us3DG6_BT_pG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize K Clusters for K = 10 and random_state = 42\n"
      ],
      "metadata": {
        "id": "6oM8LvdIfK6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(2) \n",
        "#Transform the data\n",
        "df = pca.fit_transform(X)"
      ],
      "metadata": {
        "id": "xl_QigqEUBcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Analyzing the input data in 2D based on its true labels\n",
        "\n",
        "u_labels = np.unique(y[:, 0])\n",
        "\n",
        "for i in u_labels:\n",
        "    plt.scatter(df[y[:, 0] == i , 0] , df[y[:, 0] == i , 1] , label = i)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IkMY_K8gUDeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  Submit the output plot as a part of the solutions\n",
        "'''\n",
        "\n",
        "kmeans = KMeans(n_clusters = 10, init = 'random', random_state = 42)\n",
        "'''\n",
        "  Write one line of code to get the predicted labels of the 10-clusters\n",
        "'''\n",
        "\n",
        "u_labels = np.unique(label)\n",
        "#plotting the results:\n",
        " \n",
        "for i in u_labels:\n",
        "    '''\n",
        "      Write one line of code to get a scatter plot for i-th cluster.\n",
        "      Have its label = i\n",
        "    '''\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BDq3Bd5rUJGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2: Backprop in a Neural Network "
      ],
      "metadata": {
        "id": "hOk9wtE1f0lP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Load fashionMNIST. This is the same code with homework one.\n",
        "## Part a: Load and visualize the data.\n",
        "X_train_and_val = np.load('/content/drive/My Drive/CSM146-Spring2022-HW1-code/data/binary_classification/X_train.npy')\n",
        "y_train_and_val = np.load('/content/drive/My Drive/CSM146-Spring2022-HW1-code/data/binary_classification/y_train.npy')\n",
        "X_test = np.load('/content/drive/My Drive/CSM146-Spring2022-HW1-code/data/binary_classification/X_test.npy')\n",
        "y_test = np.load('/content/drive/My Drive/CSM146-Spring2022-HW1-code/data/binary_classification/y_test.npy')\n",
        "X_train, X_val = X_train_and_val[:4500], X_train_and_val[4500:]\n",
        "y_train, y_val = y_train_and_val[:4500], y_train_and_val[4500:]\n",
        "y_train[y_train[:] == -1] = 0\n",
        "y_val[y_val[:] == -1] = 0\n",
        "y_test[y_test[:] == -1] = 0\n",
        "y_train = y_train.squeeze(1)\n",
        "y_val = y_val.squeeze(1)\n",
        "y_test = y_test.squeeze(1)\n",
        "print('Train data shape: ', X_train.shape)\n",
        "print('Train target shape: ', y_train.shape)\n",
        "print('Val data shape: ', X_val.shape)\n",
        "print('Val target shape: ', y_val.shape)\n",
        "print('Test data shape: ',X_test.shape)\n",
        "print('Test target shape: ',y_test.shape)"
      ],
      "metadata": {
        "id": "MX932ASVgB4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Part(a): Load and visualize the data.\n",
        "index = 11\n",
        "X = np.array(X_train[index], dtype='uint8')\n",
        "X = X.reshape((28, 28))\n",
        "fig = plt.figure()\n",
        "plt.imshow(X, cmap='gray')\n",
        "plt.show()\n",
        "if y_train[index] == 1:\n",
        "    label = 'Dress'\n",
        "else:\n",
        "    label = 'Shirt'\n",
        "print('label is', label)"
      ],
      "metadata": {
        "id": "ImOeXkGdZITO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class TwoLayerNet(object):\n",
        "    \"\"\"\n",
        "    A two-layer fully-connected neural network for binary classification. \n",
        "    We train the network with a softmax loss function and L2 regularization on the\n",
        "    weight matrices. The network uses a ReLU nonlinearity after the first fully\n",
        "    connected layer.\n",
        "    Input: X\n",
        "    Hidden states for layer 1: h1 = W1X + b1\n",
        "    Activations: a2 = ReLU(h1)\n",
        "    Hidden states for layer 2: h2 = W2a2 + b2\n",
        "    Probabilities: s = softmax(h2)\n",
        "    \n",
        "    ReLU function: \n",
        "    (i) x = x if x >= 0  (ii) x = 0 if x < 0\n",
        "\n",
        "    The outputs of the second fully-connected layer are the scores for each class.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n",
        "        \"\"\"\n",
        "        Initialize the model. Weights are initialized to small random values and\n",
        "        biases are initialized to zero. Weights and biases are stored in the\n",
        "        variable self.params, which is a dictionary with the following keys:\n",
        "\n",
        "        W1: First layer weights; has shape (H, D)\n",
        "        b1: First layer biases; has shape (H,)\n",
        "        W2: Second layer weights; has shape (C, H)\n",
        "        b2: Second layer biases; has shape (C,)\n",
        "\n",
        "        Inputs:\n",
        "        - input_size: The dimension D of the input data.\n",
        "        - hidden_size: The number of neurons H in the hidden layer.\n",
        "        - output_size: The number of classes C.\n",
        "        \"\"\"\n",
        "        self.params = {}\n",
        "        self.params['W1'] = std * np.random.randn(hidden_size, input_size)\n",
        "        self.params['b1'] = np.zeros(hidden_size)\n",
        "        self.params['W2'] = std * np.random.randn(output_size, hidden_size)\n",
        "        self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "    def loss(self, X, y=None, reg=0.0):\n",
        "        \"\"\"\n",
        "        Compute the loss and gradients for a two layer fully connected neural\n",
        "        network.\n",
        "\n",
        "        Inputs:\n",
        "        - X: Input data of shape (N, D). Each X[i] is a training sample.\n",
        "        - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is\n",
        "          an integer in the range 0 <= y[i] < C. This parameter is optional; if it\n",
        "          is not passed then we only return scores, and if it is passed then we\n",
        "          instead return the loss and gradients.\n",
        "        - reg: Regularization strength.\n",
        "\n",
        "        Returns:\n",
        "        If y is None, return a matrix scores of shape (N, C) where scores[i, c] is\n",
        "        the score for class c on input X[i].\n",
        "\n",
        "        If y is not None, instead return a tuple of:\n",
        "        - loss: Loss (data loss and regularization loss) for this batch of training\n",
        "          samples.\n",
        "        - grads: Dictionary mapping parameter names to gradients of those parameters\n",
        "          with respect to the loss function; has the same keys as self.params.\n",
        "        \"\"\"\n",
        "        # Unpack variables from the params dictionary\n",
        "        W1, b1 = self.params['W1'], self.params['b1']\n",
        "        W2, b2 = self.params['W2'], self.params['b2']\n",
        "        N, D = X.shape\n",
        "\n",
        "        # Compute the forward pass\n",
        "        scores = None\n",
        "\n",
        "        ### ========== TODO : START ========== ###\n",
        "        #   Calculate the output of the neural network using forward pass.  \n",
        "        #   The result should be (N, C), where N is the number of examples, and C is the number of classes. \n",
        "        #   The output of the second fully connected layer is the output scores (before softmax). \n",
        "        #   Do not use a for loop in your implementation.\n",
        "        #   Please use 'h1' as input of hidden layers, and 'a2' as output of \n",
        "        #   Refer to the comments at the beginning of this class for the model architecture\n",
        "        #   You may simply use np.maximun for implementing ReLU.\n",
        "        ##  Part (b): Implement the forward pass.\n",
        "\n",
        "\n",
        "        ### ========== TODO : END ========== ###\n",
        "\n",
        "\n",
        "        # If the targets are not given then jump out, we're done\n",
        "        if y is None:\n",
        "            return scores\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = None\n",
        "\n",
        "        # scores is num_examples by num_classes (N, C)\n",
        "        def softmax_loss(x, y):\n",
        "            ### ========== TODO : START ========== ###\n",
        "            #   Calculate the cross entropy loss after softmax output layer.\n",
        "            #   This function should return loss and dx\n",
        "            probs = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "            probs /= np.sum(probs, axis=1, keepdims=True)\n",
        "            N = x.shape[0]\n",
        "            ##  Part (d): Implement the CrossEntropyLoss\n",
        "            loss = None\n",
        "            ##  Part (d): Implement the gradient of y wrt x\n",
        "            dx = None\n",
        "\n",
        "            \n",
        "            ### ========== TODO : END ========== ###\n",
        "            return loss, dx\n",
        "        \n",
        "        \n",
        "        data_loss, dscore = softmax_loss(scores, y) \n",
        "        \n",
        "        ### ========== TODO : START ========== ###\n",
        "        #   Calculate the regularization loss. Multiply the regularization\n",
        "        #   loss by 0.5 (in addition to the factor reg).\n",
        "        ##  Part (c): Implement the regularization loss\n",
        "        \n",
        "        ### ========== TODO : END ========== ###\n",
        "\n",
        "        loss = data_loss + reg_loss\n",
        "\n",
        "        grads = {}\n",
        "\n",
        "        ### ========== TODO : START ========== ###\n",
        "        #  Compute backpropagation\n",
        "        #  Remember the loss contains two parts: cross-entropy and regularization. The computation for gradients of W1 and b1 shown here can be regarded as a reference.\n",
        "        ## Part (e): Implement the computations of gradients for W2 and b2.\n",
        "        grads['W2'] = None\n",
        "        grads['b2'] = None\n",
        "        \n",
        "        da_h = np.zeros(h1.shape)\n",
        "        da_h[h1>0] = 1\n",
        "        dh = (dscore.dot(W2) * da_h)\n",
        "\n",
        "        grads['W1'] = np.dot(dh.T,X) + reg * W1\n",
        "        grads['b1'] = np.ones(N).dot(dh)\n",
        "        ### ========== TODO : END ========== ###\n",
        "\n",
        "        return loss, grads\n",
        "\n",
        "    def train(self, X, y, X_val, y_val,\n",
        "            learning_rate=1e-3, learning_rate_decay=0.95,\n",
        "            reg=1e-5, num_iters=100,\n",
        "            batch_size=200, verbose=False):\n",
        "        \"\"\"\n",
        "        Train this neural network using stochastic gradient descent.\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (N, D) giving training data.\n",
        "        - y: A numpy array f shape (N,) giving training labels; y[i] = c means that\n",
        "          X[i] has label c, where 0 <= c < C.\n",
        "        - X_val: A numpy array of shape (N_val, D) giving validation data.\n",
        "        - y_val: A numpy array of shape (N_val,) giving validation labels.\n",
        "        - learning_rate: Scalar giving learning rate for optimization.\n",
        "        - learning_rate_decay: Scalar giving factor used to decay the learning rate\n",
        "          after each epoch.\n",
        "        - reg: Scalar giving regularization strength.\n",
        "        - num_iters: Number of steps to take when optimizing.\n",
        "        - batch_size: Number of training examples to use per step.\n",
        "        - verbose: boolean; if true print progress during optimization.\n",
        "        \"\"\"\n",
        "        num_train = X.shape[0]\n",
        "        iterations_per_epoch = max(num_train / batch_size, 1)\n",
        "\n",
        "        # Use SGD to optimize the parameters in self.model\n",
        "        loss_history = []\n",
        "        train_acc_history = []\n",
        "        val_acc_history = []\n",
        "\n",
        "        for it in np.arange(num_iters):\n",
        "            X_batch = None\n",
        "            y_batch = None\n",
        "\n",
        "            #   Create a minibatch (X_batch, y_batch) by sampling batch_size \n",
        "            #   samples randomly.\n",
        "\n",
        "            b_index = np.random.choice(num_train, batch_size)\n",
        "            X_batch = X[b_index]\n",
        "            y_batch = y[b_index]\n",
        "\n",
        "            # Compute loss and gradients using the current minibatch\n",
        "            loss, grads = self.loss(X_batch, y=y_batch, reg=reg)\n",
        "            loss_history.append(loss)\n",
        "\n",
        "            \n",
        "            self.params['W1'] -= learning_rate * grads['W1']\n",
        "            self.params['b1'] -= learning_rate * grads['b1']\n",
        "            self.params['W2'] -= learning_rate * grads['W2']\n",
        "            self.params['b2'] -= learning_rate * grads['b2']\n",
        "\n",
        "\n",
        "            if verbose and it % 100 == 0:\n",
        "                print('iteration {} / {}: loss {}'.format(it, num_iters, loss))\n",
        "\n",
        "            # Every epoch, check train and val accuracy and decay learning rate.\n",
        "            if it % iterations_per_epoch == 0:\n",
        "                # Check accuracy\n",
        "                train_acc = (self.predict(X_batch) == y_batch).mean()\n",
        "                val_acc = (self.predict(X_val) == y_val).mean()\n",
        "                train_acc_history.append(train_acc)\n",
        "                val_acc_history.append(val_acc)\n",
        "\n",
        "                # Decay learning rate\n",
        "                learning_rate *= learning_rate_decay\n",
        "\n",
        "        return {\n",
        "          'loss_history': loss_history,\n",
        "          'train_acc_history': train_acc_history,\n",
        "          'val_acc_history': val_acc_history,\n",
        "        }\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Use the trained weights of this two-layer network to predict labels for\n",
        "        data points. For each data point we predict scores for each of the C\n",
        "        classes, and assign each data point to the class with the highest score.\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (N, D) giving N D-dimensional data points to\n",
        "          classify.\n",
        "\n",
        "        Returns:\n",
        "        - y_pred: A numpy array of shape (N,) giving predicted labels for each of\n",
        "          the elements of X. For all i, y_pred[i] = c means that X[i] is predicted\n",
        "          to have class c, where 0 <= c < C.\n",
        "        \"\"\"\n",
        "        y_pred = None\n",
        "\n",
        "        ### ========== TODO : START ========== ###\n",
        "        #   Predict the class given the input data.\n",
        "        ##  Part (f): Implement the prediction function\n",
        "        pass\n",
        "        \n",
        "        ### ========== TODO : END ========== ###\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qpdpzbxaDYpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 784\n",
        "hidden_size = 50\n",
        "num_classes = 2\n",
        "net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
        "\n",
        "# Train the network\n",
        "for learning_rate in [1e-5, 1e-4, 1e-3, 5e-3, 1e-1]:\n",
        "  stats = net.train(X_train, y_train, X_val, y_val,\n",
        "              num_iters=1000, batch_size=200,\n",
        "              learning_rate=learning_rate, learning_rate_decay=0.95,\n",
        "              reg=0.1, verbose=True)\n",
        "\n",
        "  # Predict on the validation set\n",
        "  val_acc = (net.predict(X_val) == y_val).mean()\n",
        "  print('learning_rate: ', learning_rate)\n",
        "  print('Validation accuracy: ', val_acc)\n",
        "\n",
        "  # Save this net as the variable subopt_net for later comparison.\n",
        "  subopt_net = net\n",
        "  test_acc = (subopt_net.predict(X_test) == y_test).mean()\n",
        "  print('Test accuracy (subopt_net): ', test_acc)"
      ],
      "metadata": {
        "id": "TqQtNxY9Dyur"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}