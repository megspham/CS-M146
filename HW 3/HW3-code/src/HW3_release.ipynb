{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3_release.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7H420KxmCjKH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To add your own Drive Run this cell.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BCDUFCh7Fd-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Please append your own directory after â€˜/content/drive/My Drive/'\n",
        "### ========== TODO : START ========== ###\n",
        "sys.path += ['/content/drive/My Drive/CSM146-S22-HW3-code'] \n",
        "### ========== TODO : END ========== ###"
      ],
      "metadata": {
        "id": "nQXiXrbaF3NK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Author      : Yi-Chieh Wu, Sriram Sankararman\n",
        "Description : Twitter\n",
        "\"\"\"\n",
        "\n",
        "from string import punctuation\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# !!! MAKE SURE TO USE SVC.decision_function(X), NOT SVC.predict(X) !!!\n",
        "# (this makes ``continuous-valued'' predictions)\n",
        "from sklearn.svm import SVC\n",
        "#from sklearn.cross_validation import StratifiedKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "7_OLupUPC2U3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 4: Twitter Analysis Using SVM"
      ],
      "metadata": {
        "id": "47L2XVzBX6c5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "# functions -- input/output\n",
        "######################################################################\n",
        "\n",
        "def read_vector_file(fname):\n",
        "    \"\"\"\n",
        "    Reads and returns a vector from a file.\n",
        "\n",
        "    Parameters\n",
        "    --------------------\n",
        "        fname  -- string, filename\n",
        "\n",
        "    Returns\n",
        "    --------------------\n",
        "        labels -- numpy array of shape (n,)\n",
        "                    n is the number of non-blank lines in the text file\n",
        "    \"\"\"\n",
        "    return np.genfromtxt(fname)\n",
        "\n",
        "\n",
        "def write_label_answer(vec, outfile):\n",
        "    \"\"\"\n",
        "    Writes your label vector to the given file.\n",
        "\n",
        "    Parameters\n",
        "    --------------------\n",
        "        vec     -- numpy array of shape (n,) or (n,1), predicted scores\n",
        "        outfile -- string, output filename\n",
        "    \"\"\"\n",
        "\n",
        "    # for this project, you should predict 70 labels\n",
        "    if(vec.shape[0] != 70):\n",
        "        print(\"Error - output vector should have 70 rows.\")\n",
        "        print(\"Aborting write.\")\n",
        "        return\n",
        "\n",
        "    np.savetxt(outfile, vec)\n",
        "    "
      ],
      "metadata": {
        "id": "9Z8E5YL0CzWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "# functions -- feature extraction\n",
        "######################################################################\n",
        "\n",
        "def extract_words(input_string):\n",
        "    \"\"\"\n",
        "    Processes the input_string, separating it into \"words\" based on the presence\n",
        "    of spaces, and separating punctuation marks into their own words.\n",
        "\n",
        "    Parameters\n",
        "    --------------------\n",
        "        input_string -- string of characters\n",
        "\n",
        "    Returns\n",
        "    --------------------\n",
        "        words        -- list of lowercase \"words\"\n",
        "    \"\"\"\n",
        "\n",
        "    for c in punctuation :\n",
        "        input_string = input_string.replace(c, ' ' + c + ' ')\n",
        "    return input_string.lower().split()\n",
        "\n",
        "\n",
        "def extract_dictionary(infile):\n",
        "    \"\"\"\n",
        "    Given a filename, reads the text file and builds a dictionary of unique\n",
        "    words/punctuations.\n",
        "\n",
        "    Parameters\n",
        "    --------------------\n",
        "        infile    -- string, filename\n",
        "\n",
        "    Returns\n",
        "    --------------------\n",
        "        word_list -- dictionary, (key, value) pairs are (word, index)\n",
        "    \"\"\"\n",
        "\n",
        "    word_list = {}\n",
        "    idx = 0\n",
        "    with open(infile, 'r') as fid :\n",
        "        ### ========== TODO : START ========== ###\n",
        "        # part 1a: process each line to populate word_list\n",
        "\n",
        "\n",
        "        ### ========== TODO : END ========== ###\n",
        "\n",
        "    return word_list\n",
        "\n",
        "\n",
        "def extract_feature_vectors(infile, word_list):\n",
        "    \"\"\"\n",
        "    Produces a bag-of-words representation of a text file specified by the\n",
        "    filename infile based on the dictionary word_list.\n",
        "\n",
        "    Parameters\n",
        "    --------------------\n",
        "        infile         -- string, filename\n",
        "        word_list      -- dictionary, (key, value) pairs are (word, index)\n",
        "\n",
        "    Returns\n",
        "    --------------------\n",
        "        feature_matrix -- numpy array of shape (n,d)\n",
        "                          boolean (0,1) array indicating word presence in a string\n",
        "                            n is the number of non-blank lines in the text file\n",
        "                            d is the number of unique words in the text file\n",
        "    \"\"\"\n",
        "\n",
        "    num_lines = sum(1 for line in open(infile,'rU'))\n",
        "    num_words = len(word_list)\n",
        "    feature_matrix = np.zeros((num_lines, num_words))\n",
        "\n",
        "    with open(infile, 'r') as fid :\n",
        "        ### ========== TODO : START ========== ###\n",
        "        # part 1b: process each line to populate feature_matrix\n",
        "\n",
        "        ### ========== TODO : END ========== ###\n",
        "\n",
        "    return feature_matrix"
      ],
      "metadata": {
        "id": "i67aTAmrGGHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "# functions -- evaluation\n",
        "######################################################################\n",
        "\n",
        "def performance(y_true, y_pred, metric=\"accuracy\"):\n",
        "    \"\"\"\n",
        "    Calculates the performance metric based on the agreement between the\n",
        "    true labels and the predicted labels.\n",
        "\n",
        "    Parameters\n",
        "    --------------------\n",
        "        y_true -- numpy array of shape (n,), known labels\n",
        "        y_pred -- numpy array of shape (n,), (continuous-valued) predictions\n",
        "        metric -- string, option used to select the performance measure\n",
        "                  options: 'accuracy', 'f1-score', 'auroc', 'precision',\n",
        "                           'sensitivity', 'specificity'\n",
        "\n",
        "    Returns\n",
        "    --------------------\n",
        "        score  -- float, performance score\n",
        "    \"\"\"\n",
        "    # map continuous-valued predictions to binary labels\n",
        "    y_label = np.sign(y_pred)\n",
        "    y_label[y_label==0] = 1\n",
        "\n",
        "    ### ========== TODO : START ========== ###\n",
        "    # part 2a: compute classifier performance\n",
        "\n",
        "    ### ========== TODO : END ========== ###\n",
        "\n",
        "\n",
        "def cv_performance(clf, X, y, kf, metric=\"accuracy\"):\n",
        "    \"\"\"\n",
        "    Splits the data, X and y, into k-folds and runs k-fold cross-validation.\n",
        "    Trains classifier on k-1 folds and tests on the remaining fold.\n",
        "    Calculates the k-fold cross-validation performance metric for classifier\n",
        "    by averaging the performance across folds.\n",
        "\n",
        "    Parameters\n",
        "    --------------------\n",
        "        clf    -- classifier (instance of SVC)\n",
        "        X      -- numpy array of shape (n,d), feature vectors\n",
        "                    n = number of examples\n",
        "                    d = number of features\n",
        "        y      -- numpy array of shape (n,), binary labels {1,-1}\n",
        "        kf     -- cross_validation.KFold or cross_validation.StratifiedKFold\n",
        "        metric -- string, option used to select performance measure\n",
        "\n",
        "    Returns\n",
        "    --------------------\n",
        "        score   -- float, average cross-validation performance across k folds\n",
        "    \"\"\"\n",
        "\n",
        "    ### ========== TODO : START ========== ###\n",
        "    # part 2b: compute average cross-validation performance\n",
        "\n",
        "    ### ========== TODO : END ========== ###\n",
        "\n",
        "\n",
        "def select_param_linear(X, y, kf, metric=\"accuracy\"):\n",
        "    \"\"\"\n",
        "    Sweeps different settings for the hyperparameter of a linear-kernel SVM,\n",
        "    calculating the k-fold CV performance for each setting, then selecting the\n",
        "    hyperparameter that 'maximize' the average k-fold CV performance.\n",
        "\n",
        "    Parameters\n",
        "    --------------------\n",
        "        X      -- numpy array of shape (n,d), feature vectors\n",
        "                    n = number of examples\n",
        "                    d = number of features\n",
        "        y      -- numpy array of shape (n,), binary labels {1,-1}\n",
        "        kf     -- cross_validation.KFold or cross_validation.StratifiedKFold\n",
        "        metric -- string, option used to select performance measure\n",
        "\n",
        "    Returns\n",
        "    --------------------\n",
        "        C -- float, optimal parameter value for linear-kernel SVM\n",
        "    \"\"\"\n",
        "\n",
        "    print('Linear SVM Hyperparameter Selection based on ' + str(metric) + ':')\n",
        "    C_range = 10.0 ** np.arange(-3, 3)\n",
        "\n",
        "    ### ========== TODO : START ========== ###\n",
        "    # part 2c: select optimal hyperparameter using cross-validation\n",
        "\n",
        "    ### ========== TODO : END ========== ###\n",
        "\n",
        "\n",
        "def select_param_rbf(X, y, kf, metric=\"accuracy\"):\n",
        "    \"\"\"\n",
        "    Sweeps different settings for the hyperparameters of an RBF-kernel SVM,\n",
        "    calculating the k-fold CV performance for each setting, then selecting the\n",
        "    hyperparameters that 'maximize' the average k-fold CV performance.\n",
        "\n",
        "    Parameters\n",
        "    --------------------\n",
        "        X       -- numpy array of shape (n,d), feature vectors\n",
        "                     n = number of examples\n",
        "                     d = number of features\n",
        "        y       -- numpy array of shape (n,), binary labels {1,-1}\n",
        "        kf     -- cross_validation.KFold or cross_validation.StratifiedKFold\n",
        "        metric  -- string, option used to select performance measure\n",
        "\n",
        "    Returns\n",
        "    --------------------\n",
        "        gamma, C -- tuple of floats, optimal parameter values for an RBF-kernel SVM\n",
        "    \"\"\"\n",
        "\n",
        "    print('RBF SVM Hyperparameter Selection based on ' + str(metric) + ':')\n",
        "\n",
        "    ### ========== TODO : START ========== ###\n",
        "    # part 3b: create grid, then select optimal hyperparameters using cross-validation\n",
        "\n",
        "\n",
        "    ### ========== TODO : END ========== ###\n",
        "\n",
        "\n",
        "def performance_test(clf, X, y, metric=\"accuracy\"):\n",
        "    \"\"\"\n",
        "    Estimates the performance of the classifier using the 95% CI.\n",
        "\n",
        "    Parameters\n",
        "    --------------------\n",
        "        clf          -- classifier (instance of SVC)\n",
        "                          [already fit to data]\n",
        "        X            -- numpy array of shape (n,d), feature vectors of test set\n",
        "                          n = number of examples\n",
        "                          d = number of features\n",
        "        y            -- numpy array of shape (n,), binary labels {1,-1} of test set\n",
        "        metric       -- string, option used to select performance measure\n",
        "\n",
        "    Returns\n",
        "    --------------------\n",
        "        score        -- float, classifier performance\n",
        "        lower, upper -- tuple of floats, confidence interval\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    ### ========== TODO : START ========== ###\n",
        "    # part 4b: return the values of test results under a metric.\n",
        "\n",
        "    ### ========== TODO : END ========== ###"
      ],
      "metadata": {
        "id": "-MvTxQPRGOOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "# main\n",
        "######################################################################\n",
        "\n",
        "def main() :\n",
        "    np.random.seed(1234)\n",
        "\n",
        "    # read the tweets and its labels, change the following two lines to your own path.\n",
        "    file_path = '/content/drive/My Drive/CSM146-S22-HW3-code/tweets.txt'\n",
        "    label_path = '/content/drive/My Drive/CSM146-S22-HW3-code/labels.txt'\n",
        "    dictionary = extract_dictionary(file_path)\n",
        "    print(len(dictionary))\n",
        "    X = extract_feature_vectors(file_path, dictionary)\n",
        "    y = read_vector_file(label_path)\n",
        "\n",
        "\n",
        "    metric_list = [\"accuracy\", \"f1_score\", \"auroc\", \"precision\", \"sensitivity\", \"specificity\"]\n",
        "\n",
        "    ### ========== TODO : START ========== ###\n",
        "    # part 1c: split data into training (training + cross-validation) and testing set\n",
        "\n",
        "\n",
        "    # part 2b: create stratified folds (5-fold CV)\n",
        "\n",
        "    # part 2d: for each metric, select optimal hyperparameter for linear-kernel SVM using CV\n",
        "\n",
        "\n",
        "    # part 3c: for each metric, select optimal hyperparameter for RBF-SVM using CV\n",
        "\n",
        "\n",
        "    # part 4a: train linear- and RBF-kernel SVMs with selected hyperparameters\n",
        "\n",
        "\n",
        "    # part 4c: test the performance of your two classifiers.\n",
        "    \n",
        "    ### ========== TODO : END ========== ###\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\" :\n",
        "    main()"
      ],
      "metadata": {
        "id": "zMIQRGpYErVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 5: Boosting vs. Decision Tree"
      ],
      "metadata": {
        "id": "_W-_mjX0JMes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_val_score, train_test_split"
      ],
      "metadata": {
        "id": "0uzCdPTkOQSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Data :\n",
        "    \n",
        "    def __init__(self) :\n",
        "        \"\"\"\n",
        "        Data class.\n",
        "        \n",
        "        Attributes\n",
        "        --------------------\n",
        "            X -- numpy array of shape (n,d), features\n",
        "            y -- numpy array of shape (n,), targets\n",
        "        \"\"\"\n",
        "                \n",
        "        # n = number of examples, d = dimensionality\n",
        "        self.X = None\n",
        "        self.y = None\n",
        "        \n",
        "        self.Xnames = None\n",
        "        self.yname = None\n",
        "    \n",
        "    def load(self, filename, header=0, predict_col=-1) :\n",
        "        \"\"\"Load csv file into X array of features and y array of labels.\"\"\"\n",
        "        \n",
        "        # determine filename\n",
        "        f = filename\n",
        "        \n",
        "        # load data\n",
        "        with open(f, 'r') as fid :\n",
        "            data = np.loadtxt(fid, delimiter=\",\", skiprows=header)\n",
        "        \n",
        "        # separate features and labels\n",
        "        if predict_col is None :\n",
        "            self.X = data[:,:]\n",
        "            self.y = None\n",
        "        else :\n",
        "            if data.ndim > 1 :\n",
        "                self.X = np.delete(data, predict_col, axis=1)\n",
        "                self.y = data[:,predict_col]\n",
        "            else :\n",
        "                self.X = None\n",
        "                self.y = data[:]\n",
        "        \n",
        "        # load feature and label names\n",
        "        if header != 0:\n",
        "            with open(f, 'r') as fid :\n",
        "                header = fid.readline().rstrip().split(\",\")\n",
        "                \n",
        "            if predict_col is None :\n",
        "                self.Xnames = header[:]\n",
        "                self.yname = None\n",
        "            else :\n",
        "                if len(header) > 1 :\n",
        "                    self.Xnames = np.delete(header, predict_col)\n",
        "                    self.yname = header[predict_col]\n",
        "                else :\n",
        "                    self.Xnames = None\n",
        "                    self.yname = header[0]\n",
        "        else:\n",
        "            self.Xnames = None\n",
        "            self.yname = None\n",
        "\n",
        "\n",
        "# helper functions\n",
        "def load_data(filename, header=0, predict_col=-1) :\n",
        "    \"\"\"Load csv file into Data class.\"\"\"\n",
        "    data = Data()\n",
        "    data.load(filename, header=header, predict_col=predict_col)\n",
        "    return data"
      ],
      "metadata": {
        "id": "DVxef2sxOmVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the path to your own data directory\n",
        "titanic = load_data(\"/content/drive/My Drive/CSM146-S22-HW3-code/titanic_train.csv\", header=1, predict_col=0)\n",
        "X = titanic.X; Xnames = titanic.Xnames\n",
        "y = titanic.y; yname = titanic.yname\n",
        "n,d = X.shape  # n = number of examples, d =  number of features"
      ],
      "metadata": {
        "id": "_Zcf4WVqJSpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def error(clf, X, y, ntrials=100, test_size=0.2) :\n",
        "    \"\"\"\n",
        "    Computes the classifier error over a random split of the data,\n",
        "    averaged over ntrials runs.\n",
        "\n",
        "    Parameters\n",
        "    --------------------\n",
        "        clf         -- classifier\n",
        "        X           -- numpy array of shape (n,d), features values\n",
        "        y           -- numpy array of shape (n,), target classes\n",
        "        ntrials     -- integer, number of trials\n",
        "\n",
        "    Returns\n",
        "    --------------------\n",
        "        train_error -- float, training error\n",
        "        test_error  -- float, test error\n",
        "    \"\"\"\n",
        "\n",
        "    train_error = 0\n",
        "    test_error = 0\n",
        "\n",
        "    train_scores = []; test_scores = [];\n",
        "    for i in range(ntrials):\n",
        "        xtrain, xtest, ytrain, ytest = train_test_split (X,y, test_size = test_size, random_state = i)\n",
        "        clf.fit (xtrain, ytrain)\n",
        "\n",
        "        ypred = clf.predict (xtrain)\n",
        "        err = 1 - metrics.accuracy_score (ytrain, ypred, normalize = True)\n",
        "        train_scores.append (err)\n",
        "\n",
        "        ypred = clf.predict (xtest)\n",
        "        err = 1 - metrics.accuracy_score (ytest, ypred, normalize = True)\n",
        "        test_scores.append (err)\n",
        "\n",
        "    train_error =  np.mean (train_scores)\n",
        "    test_error = np.mean (test_scores)\n",
        "    return train_error, test_error\n"
      ],
      "metadata": {
        "id": "3Ta7XHRWQGNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### ========== TODO : START ========== ###\n",
        "# Part 5(a): Implement the decision tree classifier and report the training error.\n",
        "print('Classifying using Decision Tree...')\n",
        "\n",
        "### ========== TODO : END ========== ###"
      ],
      "metadata": {
        "id": "W8-U3un5PjGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_error, test_error = error (DecisionTreeClassifier (criterion = 'entropy'), X, y)\n",
        "print('\\tDecision Tree\\t-- avg train error : %.3f\\tavg test error : %.3f' %(train_error, test_error))"
      ],
      "metadata": {
        "id": "VBYsOn49P_Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### ========== TODO : START ========== ###\n",
        "# Part 5(b): Implement the random forest classifier and adjust the number of samples used in bootstrap sampling.\n",
        "\n",
        "### ========== TODO : END ========== ###"
      ],
      "metadata": {
        "id": "_x_PevK8Q4dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### ========== TODO : START ========== ###\n",
        "# Part 5(c): Implement the random forest classifier and adjust the number of features for each decision tree.\n",
        "\n",
        "### ========== TODO : END ========== ###"
      ],
      "metadata": {
        "id": "ZFUyPTPwT53v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}